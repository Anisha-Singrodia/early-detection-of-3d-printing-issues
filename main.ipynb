{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from load_data import CustomDataset, CustomTestDataset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from typing import Callable\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize, CenterCrop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer_factory: Callable[\n",
    "    [nn.Module], torch.optim.Optimizer\n",
    "] = lambda model: torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "resize = (224,224)\n",
    "\n",
    "transforms = Compose(\n",
    "    [   \n",
    "        ToTensor(),\n",
    "        # CenterCrop((480, 640)),\n",
    "        Resize(resize),\n",
    "        # Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)),\n",
    "        # Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        # RandomRotation(1),\n",
    "        # ColorJitter(brightness = 0.1, contrast = 0.1, saturation = 0.1),\n",
    "        # Normalize(mean=[0.485, 0.456, 0.4], std=[0.229, 0.224, 0.2])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, num_channels, resize, transform = None, device = \"cpu\"):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.device = device\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.img_tensor, self.label_tensor = self.load_data(num_channels, resize)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.img_tensor[index], self.label_tensor[index])\n",
    "    \n",
    "    def load_data(self, num_channels, resize):\n",
    "        # img_tensor = torch.zeros((len(self.df), num_channels, resize[0], resize[1])).to(self.device)\n",
    "        # label_tensor = torch.zeros((len(self.df)))\n",
    "        for i in range(len(self.df)):\n",
    "            img_name = os.path.join(self.images_folder, self.df.iloc[i, 0])\n",
    "            image = cv2.imread(img_name)\n",
    "            label = torch.tensor(int(self.df.iloc[i, -1])).to(self.device)\n",
    "            if self.transform:\n",
    "                image = self.transform(image).to(self.device)\n",
    "            if i==0:\n",
    "                img_tensor = image\n",
    "                label_tensor = [label]\n",
    "                # print(label_tensor)\n",
    "            else:\n",
    "                img_tensor = torch.stack((img_tensor, torch.unsqueeze(image, 0)), 0)\n",
    "                label_tensor.append([label])\n",
    "            print(img_tensor.shape)    \n",
    "        return img_tensor, torch.tensor(label_tensor).to(self.device)\n",
    "    \n",
    "    def load_data1(self):\n",
    "        f = open(\"./train.csv\", \"r\")\n",
    "        l = f.readlines()\n",
    "        # img_path_list = glob(data_path)\n",
    "        for i in range(1, len(l)):\n",
    "            line = l[i].split(\",\")\n",
    "            img_path = line[0]\n",
    "            lab = torch.tensor(int(line[-1].strip()))\n",
    "            # tranform the image to same size\n",
    "            image = cv2.imread(os.path.join(self.images_folder, img_path))\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            if i == 1:\n",
    "                img_tensor = image\n",
    "                label = lab\n",
    "            else:\n",
    "                img_tensor = torch.hstack((img_tensor, image))\n",
    "                label = torch.hstack((label, lab))\n",
    "        return img_tensor, label\n",
    "    \n",
    "class CustomTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, num_channels, resize, transform = None, device = \"cpu\"):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.device = device\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.img_tensor = self.load_data(num_channels, resize)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.img_tensor[index]\n",
    "    \n",
    "    def load_data(self, num_channels, resize):\n",
    "        img_tensor = torch.zeros((len(self.df), num_channels, resize[0], resize[1])).to(self.device)\n",
    "        for i in range(len(self.df)):\n",
    "            img_name = os.path.join(self.images_folder, self.df.iloc[i, 0])\n",
    "            image = cv2.imread(img_name)\n",
    "            if self.transform:\n",
    "                image = self.transform(image).to(self.device)\n",
    "            img_tensor[i,:,:,:] = image\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 3, 224, 224] at entry 0 and [3, 224, 224] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create the data loaders:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset(\u001b[39m\"\u001b[39;49m\u001b[39m./train.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m./images\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m3\u001b[39;49m, resize, transforms, \u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(CustomTestDataset(\u001b[39m\"\u001b[39m\u001b[39m./test.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m./images\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3\u001b[39m, resize, transforms), \n\u001b[1;32m      4\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m train, val \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(dataset, [\u001b[39m0.7\u001b[39m, \u001b[39m0.3\u001b[39m])\n",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, csv_path, images_folder, num_channels, resize, transform, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_folder \u001b[39m=\u001b[39m images_folder\n\u001b[1;32m      6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[0;32m----> 7\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_tensor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data(num_channels, resize)\n",
      "Cell \u001b[0;32mIn[35], line 29\u001b[0m, in \u001b[0;36mCustomDataset.load_data\u001b[0;34m(self, num_channels, resize)\u001b[0m\n\u001b[1;32m     26\u001b[0m     label_tensor \u001b[39m=\u001b[39m [label]\n\u001b[1;32m     27\u001b[0m     \u001b[39m# print(label_tensor)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     img_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack((img_tensor, image), \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m     label_tensor\u001b[39m.\u001b[39mappend([label])\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(img_tensor\u001b[39m.\u001b[39mshape)    \n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 3, 224, 224] at entry 0 and [3, 224, 224] at entry 1"
     ]
    }
   ],
   "source": [
    "# Create the data loaders:\n",
    "dataset = CustomDataset(\"./train.csv\", \"./images\", 3, resize, transforms, \"cuda\")\n",
    "test_loader = DataLoader(CustomTestDataset(\"./test.csv\", \"./images\", 3, resize, transforms), \n",
    "    batch_size=batch_size, shuffle=True)\n",
    "train, val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = Model(num_channels=3, num_classes=2)\n",
    "# Create the optimizer:\n",
    "optimizer = optimizer_factory(model)\n",
    "# Create the loss function:\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Train the model:\n",
    "tic = time.time()\n",
    "num_epochs = 10\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2593920, 32])\n",
      "torch.Size([81060])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/222 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16972 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     18\u001b[0m \u001b[39m# Loop over the training data:\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     20\u001b[0m     \u001b[39m# Move the data to the device:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     a \u001b[39m=\u001b[39m x\n\u001b[1;32m     22\u001b[0m \u001b[39m#     x, y = x.to(device), y.to(device)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#     # Zero the gradients:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#     optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m# print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}')\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# print(\"acc : \", num_correct)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/kaggle/load_data.py:31\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m     22\u001b[0m     \u001b[39m# if torch.is_tensor(index):\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39m#     index = index.tolist()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# if self.transform:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39m#     image = self.transform(image)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_tensor[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_tensor[index])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16972 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "# Move the model to the device:\n",
    "model.to(device)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc, validation_acc = [], []\n",
    "# Loop over the epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    train, val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    print(dataset.img_tensor.shape)\n",
    "    print(dataset.label_tensor.shape)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    train_loss = 0.0\n",
    "    # Set the model to training mode:\n",
    "    model.train()\n",
    "\n",
    "    # Loop over the training data:\n",
    "    for x, y in tqdm(train_loader):\n",
    "        # Move the data to the device:\n",
    "        a = x\n",
    "    #     x, y = x.to(device), y.to(device)\n",
    "    #     # Zero the gradients:\n",
    "    #     optimizer.zero_grad()\n",
    "    #     # Forward pass:\n",
    "    #     y_hat = model(x)\n",
    "    #     # Compute the loss:\n",
    "    #     loss = criterion(y_hat, y)\n",
    "    #     # Backward pass:\n",
    "    #     loss.backward()\n",
    "    #     # Update the parameters:\n",
    "    #     optimizer.step()\n",
    "    #     # Calculate Loss\n",
    "    #     train_loss += loss.item()\n",
    "    # train_loss_list.append(train_loss)\n",
    "\n",
    "    # device = \"cpu\"\n",
    "    # model.to(device)\n",
    "    # valid_loss = 0.0\n",
    "    # # Set the model to evaluation mode:\n",
    "    # model.eval()\n",
    "    # # Initialize the number of correct predictions:\n",
    "    # num_correct = 0\n",
    "    # pred = np.array([])\n",
    "    # # Loop over the data:\n",
    "    # for x, y in tqdm(val_loader):\n",
    "    #     # Move the data to the device:\n",
    "    #     x, y = x.to(device), y.to(device)\n",
    "    #     # Forward pass:\n",
    "    #     y_hat = model(x)\n",
    "    #     # Compute the predictions:\n",
    "    #     predictions = torch.argmax(y_hat, dim=1)\n",
    "    #     pred = np.hstack((pred, predictions.cpu()))\n",
    "    #     loss = criterion(y_hat,y)\n",
    "    #     valid_loss += loss.item()\n",
    "    #     num_correct += (predictions == y).float().sum().item()\n",
    "    # val_loss_list.append(valid_loss)\n",
    "    # print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}')\n",
    "    # print(\"acc : \", num_correct)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.hstack((pred, predictions.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = np.arange(num_epochs)\n",
    "# plot lines\n",
    "plt.plot(epochs, train_loss_list, label = \"line 1\")\n",
    "plt.plot(epochs, val_loss_list, label = \"line 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()\n",
    "print(\n",
    "    f\"Training time: {toc - tic:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Training time: {(toc - tic)/60:.2f} mins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode:\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# Compute the accuracy on the test data:\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode:\n",
    "model.eval()\n",
    "# Initialize the number of correct predictions:\n",
    "num_correct = 0\n",
    "pred = np.array([])\n",
    "# Loop over the data:\n",
    "for x in tqdm(test_loader):\n",
    "    # Move the data to the device:\n",
    "    x = x.to(device)\n",
    "    # Forward pass:\n",
    "    y_hat = model(x)\n",
    "    # Compute the predictions:\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    pred = np.hstack((pred, predictions))\n",
    "\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "img_path = test_df[\"img_path\"]\n",
    "df = pd.DataFrame(pred)\n",
    "df.insert(0, \"img_path\", img_path, True)\n",
    "df.to_csv(\"submission\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.astype(int)\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "img_path = test_df[\"img_path\"]\n",
    "df = pd.DataFrame()\n",
    "df[\"has_under_extrusion\"] = pred\n",
    "df.insert(0, \"img_path\", img_path, True)\n",
    "df.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f1632d6d0dbf9602470f91f75f356a1d9711039ee2302f20fe3c6a13014e7a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
