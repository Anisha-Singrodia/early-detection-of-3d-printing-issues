{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import datasets, models\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from typing import Callable\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize, CenterCrop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer_factory: Callable[\n",
    "    [nn.Module], torch.optim.Optimizer\n",
    "] = lambda model: torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "resize = (32,32)\n",
    "\n",
    "transforms = Compose(\n",
    "    [   \n",
    "        ToTensor(),\n",
    "        # CenterCrop((480, 640)),\n",
    "        Resize(resize),\n",
    "        # Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)),\n",
    "        # Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        # RandomRotation(1),\n",
    "        # ColorJitter(brightness = 0.1, contrast = 0.1, saturation = 0.1),\n",
    "        # Normalize(mean=[0.485, 0.456, 0.4], std=[0.229, 0.224, 0.2])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform = None, device = \"cpu\"):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.device = device\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.img_tensor, self.label_tensor = self.load_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.img_tensor[index], self.label_tensor[index])\n",
    "    \n",
    "    def load_data(self):\n",
    "        # img_tensor = torch.zeros((len(self.df), num_channels, resize[0], resize[1])).to(self.device)\n",
    "        # label_tensor = torch.zeros((len(self.df)))\n",
    "        for i in range(len(self.df)):\n",
    "            img_name = os.path.join(self.images_folder, self.df.iloc[i, 0])\n",
    "            image = cv2.imread(img_name)\n",
    "            label = int(self.df.iloc[i, -1])\n",
    "            if self.transform:\n",
    "                image = self.transform(image).to(self.device)\n",
    "            if i==0:\n",
    "                img_tensor = torch.unsqueeze(image, 0)\n",
    "                label_tensor = [label]\n",
    "                # print(label_tensor)\n",
    "            else:\n",
    "                img_tensor = torch.cat((img_tensor, torch.unsqueeze(image, 0)), 0)\n",
    "                label_tensor.append(label)\n",
    "            # print(img_tensor.shape)\n",
    "        return img_tensor, torch.tensor(label_tensor).to(self.device)\n",
    "    \n",
    "class CustomTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform = None, device = \"cpu\"):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.device = device\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.img_tensor = self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.img_tensor[index]\n",
    "    \n",
    "    def load_data(self):\n",
    "        for i in range(len(self.df)):\n",
    "            img_name = os.path.join(self.images_folder, self.df.iloc[i, 0])\n",
    "            image = cv2.imread(img_name)\n",
    "            if self.transform:\n",
    "                image = self.transform(image).to(self.device)\n",
    "            if i==0:\n",
    "                img_tensor = torch.unsqueeze(image, 0)\n",
    "                # print(label_tensor)\n",
    "            else:\n",
    "                img_tensor = torch.cat((img_tensor, torch.unsqueeze(image, 0)), 0)\n",
    "            # print(img_tensor.shape)\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the data loaders:\n",
    "dataset = CustomDataset(\"./train.csv\", \"./images\", transforms, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loader = DataLoader(CustomTestDataset(\"./test.csv\", \"./images\", transforms), \n",
    "    batch_size=batch_size, shuffle=True)\n",
    "# train, val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "# train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN with 2 convolutional layers and 2 fully-connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        # Input = 3 x 224 x 224, Output = 16 x 224 x 224\n",
    "        self.conv_layer1 = nn.Conv2d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        # Input = 16 x 224 x 224, Output = 16 x 224 x 224\n",
    "        self.conv_layer2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        # Input = 16 x 224 x 224, Output = 16, 112, 112\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Input = 16, 112, 112, Output = 32, 112, 112\n",
    "        self.conv_layer3 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        # Input = 32, 112, 112, Output = 32, 112, 112\n",
    "        self.conv_layer4 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        # Input = 32, 112, 112, Output = 32, 56, 56\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(4096, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        forward method\n",
    "        \"\"\"\n",
    "        out = self.conv_layer1(x)\n",
    "        # print(out.shape)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv_layer2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        \n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # print(out.shape)\n",
    "        # out = nn.Dropout(0.25)(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        # print(out.shape)\n",
    "        # out = self.soft(out)\n",
    "        # print(out.shape)\n",
    "        # exit()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = Model(num_channels=3, num_classes=2)\n",
    "# Create the optimizer:\n",
    "optimizer = optimizer_factory(model)\n",
    "# Create the loss function:\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Train the model:\n",
    "tic = time.time()\n",
    "num_epochs = 50\n",
    "num_channels = 3\n",
    "train, val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "# print(dataset.img_tensor.shape)\n",
    "# print(dataset.label_tensor.shape)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import pretrainedmodels as pm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model_ft = pm.__dict__[\"resnet50\"](pretrained='imagenet')\n",
    "\n",
    "model_ft.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "model_ft.last_linear = nn.Sequential(\n",
    "    nn.BatchNorm1d(2048),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=2048, out_features=2048),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=2048, out_features=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                data_loader, \n",
    "                dataset_size, \n",
    "                optimizer, \n",
    "                scheduler, \n",
    "                num_epochs):\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "        \n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "        \n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=10, sampler=train_subsampler)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=10, sampler=test_subsampler)\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            # Iterate over data.\n",
    "            for bi, d in (train_loader):\n",
    "                # inputs = d[\"image\"]\n",
    "                # labels = d[\"labels\"]\n",
    "                inputs = bi.to(device)\n",
    "                labels = d.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            print('train Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for bi, d in (val_loader):\n",
    "            inputs = bi.to(device)\n",
    "            labels = d.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        print('val Loss: {:.4f}'.format(epoch_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 58.8321\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 57.7373\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 53.2738\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 50.1646\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 49.3968\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 46.4615\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 44.9536\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 43.8399\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 43.5864\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 32.0414\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 31.1812\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 29.7489\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 31.1069\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 28.5849\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 27.9795\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 27.2332\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 26.4577\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 27.1511\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 26.4698\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 26.0389\n",
      "val Loss: 3.2846\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 25.4404\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 26.3253\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 27.4311\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 27.5367\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 26.4993\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 27.6030\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 26.0508\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 26.7920\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 25.9358\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 25.8815\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 26.2100\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 25.8830\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 26.1071\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 24.8587\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 26.1186\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 26.7787\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 25.4021\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 26.5192\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 25.1925\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 26.5377\n",
      "val Loss: 3.0144\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 26.1472\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 25.6099\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 24.7166\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 26.6445\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 25.5154\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 26.5191\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 25.9141\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 26.5920\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 25.3563\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 25.7215\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 25.7326\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 27.0498\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 24.9115\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 25.9044\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 27.1842\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 26.0413\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 26.5363\n",
      "Epoch 17/19\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer_ft \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(plist, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      9\u001b[0m lr_sch \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer_ft, step_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(model_ft,\n\u001b[1;32m     13\u001b[0m                        train_loader,\n\u001b[1;32m     14\u001b[0m                        \u001b[39mlen\u001b[39;49m(train_loader),\n\u001b[1;32m     15\u001b[0m                        optimizer_ft,\n\u001b[1;32m     16\u001b[0m                        lr_sch,\n\u001b[1;32m     17\u001b[0m                        num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m torch\u001b[39m.\u001b[39msave(model_ft\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodel.bin\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, dataset_size, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     44\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     47\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     48\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/pretrainedmodels/models/torchvision_models.py:340\u001b[0m, in \u001b[0;36mmodify_resnets.<locals>.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 340\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    341\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits(x)\n\u001b[1;32m    342\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/pretrainedmodels/models/torchvision_models.py:330\u001b[0m, in \u001b[0;36mmodify_resnets.<locals>.features\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    328\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    329\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 330\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m    331\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "model_ft = model_ft.to(device)\n",
    "plist = [\n",
    "        {'params': model_ft.layer4.parameters(), 'lr': 1e-5},\n",
    "        {'params': model_ft.last_linear.parameters(), 'lr': 5e-3}\n",
    "        ]\n",
    "optimizer_ft = optim.Adam(plist, lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "model_ft = train_model(model_ft,\n",
    "                       train_loader,\n",
    "                       len(train_loader),\n",
    "                       optimizer_ft,\n",
    "                       lr_sch,\n",
    "                       num_epochs=20)\n",
    "\n",
    "torch.save(model_ft.state_dict(), \"model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [01:13<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode:\n",
    "model_ft.eval()\n",
    "device = \"cpu\"\n",
    "# Compute the accuracy on the test data:\n",
    "model_ft.to(device)\n",
    "# Set the model to evaluation mode:\n",
    "model_ft.eval()\n",
    "# Initialize the number of correct predictions:\n",
    "num_correct = 0\n",
    "pred = np.array([])\n",
    "# Loop over the data:\n",
    "for x in tqdm(test_loader):\n",
    "    # Move the data to the device:\n",
    "    x = x.to(device)\n",
    "    # Forward pass:\n",
    "    y_hat = model_ft(x)\n",
    "    # Compute the predictions:\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    pred = np.hstack((pred, predictions))\n",
    "\n",
    "pred = pred.astype(int)\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "img_path = test_df[\"img_path\"]\n",
    "df = pd.DataFrame()\n",
    "df[\"has_under_extrusion\"] = pred\n",
    "df.insert(0, \"img_path\", img_path, True)\n",
    "df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the device:\n",
    "model.to(device)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc, validation_acc = [], []\n",
    "# Loop over the epochs:\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    train_loss = 0.0\n",
    "    # Set the model to training mode:\n",
    "    model.train()\n",
    "    # Loop over the training data:\n",
    "    for x, y in train_loader:\n",
    "        # Move the data to the device:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Zero the gradients:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass:\n",
    "        y_hat = model(x)\n",
    "        # Compute the loss:\n",
    "        loss = criterion(y_hat, y)\n",
    "        # Backward pass:\n",
    "        loss.backward()\n",
    "        # Update the parameters:\n",
    "        optimizer.step()\n",
    "        # Calculate Loss\n",
    "        train_loss += loss.item()\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model.to(device)\n",
    "    valid_loss = 0.0\n",
    "    # Set the model to evaluation mode:\n",
    "    model.eval()\n",
    "    # Initialize the number of correct predictions:\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    pred = np.array([])\n",
    "    # Loop over the data:\n",
    "    for x, y in val_loader:\n",
    "        # Move the data to the device:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Forward pass:\n",
    "        y_hat = model(x)\n",
    "        # Compute the predictions:\n",
    "        predictions = torch.argmax(y_hat, dim=1)\n",
    "        pred = np.hstack((pred, predictions.cpu()))\n",
    "        loss = criterion(y_hat,y)\n",
    "        valid_loss += loss.item()\n",
    "        num_correct += (predictions == y).float().sum().item()\n",
    "        total += len(predictions)\n",
    "    val_loss_list.append(valid_loss)\n",
    "    if epoch%100 == 0:\n",
    "        print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}')\n",
    "        print(\"acc : \", num_correct/total)\n",
    "    validation_acc.append(num_correct/total)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1])\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = np.arange(num_epochs)\n",
    "# plot lines\n",
    "plt.plot(epochs[1:], train_loss_list[1:], label = \"line 1\")\n",
    "plt.plot(epochs[1:], val_loss_list[1:], label = \"line 2\")\n",
    "# plt.plot(epochs, validation_acc, label = \"line 3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1309.28 seconds\n",
      "Training time: 21.82 mins\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "print(\n",
    "    f\"Training time: {toc - tic:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Training time: {(toc - tic)/60:.2f} mins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode:\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# Compute the accuracy on the test data:\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode:\n",
    "model.eval()\n",
    "# Initialize the number of correct predictions:\n",
    "num_correct = 0\n",
    "pred = np.array([])\n",
    "# Loop over the data:\n",
    "for x in tqdm(test_loader):\n",
    "    # Move the data to the device:\n",
    "    x = x.to(device)\n",
    "    # Forward pass:\n",
    "    y_hat = model(x)\n",
    "    # Compute the predictions:\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    pred = np.hstack((pred, predictions))\n",
    "\n",
    "pred = pred.astype(int)\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "img_path = test_df[\"img_path\"]\n",
    "df = pd.DataFrame()\n",
    "df[\"has_under_extrusion\"] = pred\n",
    "df.insert(0, \"img_path\", img_path, True)\n",
    "df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            print(dataloaders)\n",
    "            print(phase)\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                print(\"inside\")\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /home/singroa/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n",
      "100%|██████████| 507M/507M [00:04<00:00, 114MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 32\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 32\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, val \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(dataset, [\u001b[39m0.7\u001b[39m, \u001b[39m0.3\u001b[39m])\n\u001b[1;32m      2\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(val, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "train, val = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloaders_dict = {}\n",
    "dataloaders_dict['train'] = train_loader\n",
    "dataloaders_dict['val'] = val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x7f79beefa2b0>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f79bc0b8b50>}\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7093 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 298, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"/tmp/ipykernel_24462/2102966010.py\", line 13, in __getitem__\n    return (self.img_tensor[index], self.label_tensor[index])\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Train and evaluate\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_ft, hist \u001b[39m=\u001b[39m train_model(model_ft\u001b[39m.\u001b[39;49mto(device), dataloaders_dict, criterion, optimizer_ft, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, is_inception\u001b[39m=\u001b[39;49m(model_name\u001b[39m==\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minception\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn[92], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(dataloaders)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(phase)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m tqdm(dataloaders[phase]):\n\u001b[1;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minside\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/singroa/miniconda3/envs/cis522/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 298, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"/tmp/ipykernel_24462/2102966010.py\", line 13, in __getitem__\n    return (self.img_tensor[index], self.label_tensor[index])\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = \"cpu\"\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft.to(device), dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f1632d6d0dbf9602470f91f75f356a1d9711039ee2302f20fe3c6a13014e7a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
